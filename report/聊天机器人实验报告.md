# <center>              聊天机器人   实验报告  </center>



![](C:\Users\ASUS\Desktop\Computer_Organization_And_Design\experiment\report\ustc.png)

<div style="display: flex;flex-direction: column;align-items: center;font-size:2em">
<div>
<p>实验题目：Project 聊天机器人</p><p>学生姓名：李骋昊 吕凯盛 季严彪</p><p>学生学号：PB20081583 PB20081590 PB19081595</p><p>完成时间：2022年5月23日</p><div>
<div STYLE="page-break-after: always;"></div>




<div STYLE="page-break-after: always;"></div>

[TOC]

<div STYLE="page-break-after: always;"></div>

## 实验目的

- 基于机器学习，通过python语言训练一个实现自然语言处理的聊天机器人。

- 通过nodejs+websocket+html的方案，实现了图形界面和用户接口

- 前端和后端之间通过socket  

  

## 实现模块

- **启动模块**:即主程序bot.py
- **训练模块**：trainning.py
- **前端**：包括网页index.html,界面index.js和sokect通信模块server.js



## 代码实现

​	本次实验基于高内聚、低耦合的基本原则，将程序分为前端和后端两个部分：前端接受用户输入，并将回复输出，而后端负责产生回复。经过仔细的调研，最终前端采用 nodejs+websocket+html的方案，实现了图形界面和用户接口；而后端采用基于 intent 的神经网络模型，使用 Python实现。 nodejs 和 Python 进程之间通过 socket通信。  

### 序贯神经网络训练模型（tranning.py）

​	本次实验以自建语料库intents为核心，对语料库中不同意图对应的语句加以学习，训练得到一个简单的词袋分类器，将输入语句词袋化之后根据相似度将其分类，并输出预置类别的回复。  

```json
// 语料库样例
{
  "intends":  [
    {
  "tag": "greeting",
  "patterns": ["hi","hello"],
    "response": ["Hi! I'm your personal chatting_bot","Hey!","Hello"]
    }
      }
```

​	本部分代码的第一部分是对语料库中的输入语素通过nltk库中的word_tokenize函数进行音节的提取并用lemmatize进行词形还原，进行排序并存入输入模型中。同时需要将每种意图（即“tag”）放入待学习模型中。同时抓取成对的输入意图与对应的可能输入语料用于后续训练。

​	随后，我们根据上述一一对应关系建立词袋bags（通过0，1表示是否符合）并与学习到的意图类型对应放入训练矩阵traning中，并提取矩阵的两列向量x和y进行最后的训练。

​	最终，我们通过keras中简单的序贯模型Sequential()建立训练模型chatting_model，加入全连接层Dense和Dropout采用relu函数作为激活函数训练114514次得到我们的最终结果，注意，由于样本量不是很大，这次采用的是随机梯度下降（SGD）算法进行拟合。

​	

```python
	training_x = list(training[:, 0])
    training_y = list(training[:, 1])
#建立模型
    chatting_model = Sequential()
    chatting_model.add(Dense(128, input_shape=(len(training_x[0]),), activation='relu'))
    chatting_model.add(Dropout(0.5))
    chatting_model.add(Dense(64, activation='relu'))
    chatting_model.add(Dropout(0.5))
    chatting_model.add(Dense(len(training_y[0]), activation='softmax'))
#拟合训练
    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    chatting_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    hist = chatting_model.fit(np.array(training_x), np.array(training_y), epochs=114514, batch_size=5, verbose=1)
```

### 输入分析匹配与结果输出（bot.py)

**bot思路(后端部分)**:

1. 先对输入语料进行分类。分为如下三类a.数学语料b.中文语料c. 英文语料
2. 如果是数学语料，调用eval()函数进行计算。并且要做出异常处理，即输出错误语句。
3. 如果是中文语料，检测到第一个词为"百科"时，直接调用百度百科搜索
4. 如果是英文语料，则要使用之前的训练模型进行预测。
5. 返回得到的结果

**bot思路(前端部分)**：

1. 通过 html 输入框获取用户输入
2. 网页集成的js脚本获取输入，通过websocket送出
3. nodejs实现的websocket服务器获取用户输入，发送至后端socket服务器
4. 获取后端socket服务器返回数据，通过websocket送至网页脚本
5. js脚本通过操作dom在网页动态显示回复

**后端实现**
1. 数学语料处理

   ```python
   # 使用正则表达式进行查找
    if re.search('^[0-9+\-][0-9+\-*/\.]*', message):
               try:
                   res= message + '=' + str(eval(message))
               except:
                   res = 'In this age, still doing traditonal math?'
   ```

   先利用正则表达式进行判断，如果是数学式。那么就调用eval()，eval出现error时，就接收异常并抛出语句。

2. 中文语料处理

   ```python
           if message.split()[0] == "百科":
               r = req.get('https://baike.baidu.com/item/' + message.split()[1], headers=headers)
               try:
                   r.encoding = 'utf-8'
                   regex = re.compile('<div class="lemma-summary" label-module="lemmaSummary">(\s*)<div class="para" label-module="para">([\s\S]*?)</div>(\s*)</div>')
                   res = re.findall(regex, r.text)[0][1]
               except:
                   res = '好难啊我有点看不懂'
   ```

   先调用split方法打散message，如果发现第一个词是“百科”命令，就调用request.get()爬取后接词语百度百科的相关资料。获取到资料后，再次利用正则表达式匹配相关资料并返回。如果没有匹配到表明有误，bot表示自己并不懂

3. 英文语料处理

   ```python
   #先调用nltk库把输入的英文语料断开，再用lemmatizer将其还原为原始形式(即原词根)
   def opti_sentence(sentence):
       sentence_words = nltk.word_tokenize(sentence)
       sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
       # divide sentence and lemmatize each word
       return sentence_words
   ```

   ```python
   #构建哈希表，将每一个关键词映射到语料库的每一个tag上
   def store_words(sentence):
       # check if in the words list
       sentence_words = opti_sentence(sentence)
       container = [0]*len(words)#哈希表创建
       for word_s in sentence_words:
           for i, word in enumerate(words):
               if word_s == word:
                   container[i] = 1
               #构建映射
       return np.array(container)
   ```

   ```python
   #利用botTraining.py的训练结果预测提问的模式
   def predict(sentence):
       bow=store_words(sentence)#得到的哈希表
       res=model.predict(np.array([bow]))[0]#预测
       ERROR_THRESHOLD=0.25 #判定阈值
       results=[[i,r] for i, r in enumerate(res) if r>ERROR_THRESHOLD]
   	
       results.sort(key=lambda x:x[1],reverse=True)#把预测的各种模式从大到小排列
       return_list=[]
       for r in results:
           return_list.append({'intends':learns[r[0]], 'probability':str(r[1])})
       return return_list
   ```

   ```python
   # 在语料库(字典)中根据最可能的结果搜索回答
   def getresponse(intend_list,intend_json):
       tag=intend_list[0]['intends']
       list_intends=intend_json['intends']
       for i in list_intends:
           if i['tag']==tag:
               result=random.choice(i['response'])
               break
       return result
   ```

   英文部分的处理是现将输入的语料还原成一个原词根形式，再放入语料列表中。再构建语料库哈希表，将每一个词映射到语料库中，利用训练的结果进行预测，得到对应的询问条目。最后随机选取一个回答条目进行回答。

    **前端实现**
    1. 获取用户输入
    ```javascript
    // input事件发送数据
    submit.onclick = (e) => {
        if (e.target.innerHTML == '回复中...') {
            return false
        }
        e.target.innerHTML = '回复中...';
        const str = document.getElementById("pl").value;
        // 发送给websocket服务器
        webSocket.send(str);
        addMsg(2, str);
    }
    // 绑定回车事件
    function keyEnter() {
        if (event.keyCode == 13) {
            document.getElementById("submit").click();
        }
    }
    ```
    2. 动态显示消息列表
    ```javascript
    // 添加消息
    function addMsg(type, msg) {
        let li = document.createElement('li');
        // 1机器人/2自己
        if (type == 1) {
            li.classList.add('computer-say');
            li.innerHTML = `<img src="assets/kang.jpg"  style="width:33px; height:33px; border-radius:110%; overflow:hidden;"><span class="computer say">${msg}</span>`;
        } else {
            li.classList.add('my-say');
            li.innerHTML = `<span class="computer say">${msg}</span><img src="assets/sun.jpg" alt="我"  style="width:33px; height:33px; border-radius:110%; overflow:hidden;"></span>`;
            pl.value = '';
        }
        document.getElementById('view').appendChild(li);
        document.getElementById('ulView').scrollTo(0, document.getElementById('view').clientHeight);
    }
    ```
    3. websocket 服务器
    ```javascript
    app.ws.use(route.all('/', ctx => {
    // websocket作为“ctx.websocket”添加到上下文中。
        ctx.websocket.on('message', message => {
            // 获取到用户输入
            startRequest(message, ctx);
        });
    }));

    function startRequest(message, ctx) {
        const net = require('net');
        const port = 8001;
        const hostname = '127.0.0.1';
        const sock = new net.Socket();
        sock.setEncoding = 'UTF-8'
        // 将用户输入通过socket发送至后端
        sock.connect(port, hostname, function(){
            sock.write(message)
        });
        // 获取后端回复
	    sock.on('data', function(res){
            tx.websocket.send(res.toString());
        });
    }
    ```
## 运行结果

​	本机器人具有：简单数学计算，中文百科搜索，英文聊天三项功能。通过以下三种情况，可以得到：该机器人具有基本的对话能力和一定的话题扩展能力，满足了基本的需求；同时对话延迟控制在毫秒级别，反应灵活迅速；对于用户的意图推断较为准确，回答能够贴近主题。  

### 数学计算

![image-20220524095030577](C:\Users\77089\AppData\Roaming\Typora\typora-user-images\image-20220524095030577.png)

### 百科搜索

![image-20220524095052803](C:\Users\77089\AppData\Roaming\Typora\typora-user-images\image-20220524095052803.png)

### 英文聊天

![image-20220524095136926](C:\Users\77089\AppData\Roaming\Typora\typora-user-images\image-20220524095136926.png)

## 总结与体会

​	本次实验为程序设计 II 的综合性实验，很好的融合了字符串处理、 socket 编程、软件工程等知识，比普通实验更贴近实际，让同学们更能体会实际工程、生产中面临的挑战，让知识跳出课本，成为实际体验的一部分。就本组而言， nodejs 和神经网络都是平常未接触过的全新技术，这次试验让我们有机会探究新技术同时实践所学知识。本次实验不仅锻炼了我们的编程能力，更锻炼了我们的团队协作能力、信息检索与学习能力、遇到困难解决困难的恒心与毅力。  

​	本次实验分工为：李骋昊（PB20081583），负责训练模型的编写，语料库的完善以及加载程序的调试；吕凯盛（PB20081590），负责语料库的整体建立与加载程序的编写，并增加了计算模块。季严彪（PB19081595），负责整个前端的编写与中文百科模块的加入与相关调试工作。贡献比为1：1：1。

## 开源代码库的引用

- *tensorflow*中集成的 *keras*库
- *nltk* 中的 *stem.WordNetLemmatizer* 和 *word_tokenize* 函数
- *numpy* 库
- *request* 库
- *koa* 库

